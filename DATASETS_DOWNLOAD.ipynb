{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onanmuhumuza/IMAGE-CAPTIONING-USING-CNN-AND-LSTM/blob/main/DATASETS_DOWNLOAD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CILjLpk0ujgf"
      },
      "outputs": [],
      "source": [
        "pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4gjuT_dL_P61"
      },
      "source": [
        "**INSTALLING TENSOR FLOW**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ai3xEnPcuz08"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UqrF9q-_cZM"
      },
      "source": [
        "**UPLOADING KAGGLE API TO DOWNLOAD THE CONTENT**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hEVdakPzMVt"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX1OIf9B_tNY"
      },
      "source": [
        "**SETTING PERMISSIONS**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HYtctICdzXMP"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTYo95Yd3o8n"
      },
      "source": [
        "**DOWNLOADING hsankesara/flickr-image-dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AIu8_uIEwDjb"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d hsankesara/flickr-image-dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_5MaFX_MjAe"
      },
      "source": [
        " **Un Zipping contents of hsankesara/flickr-image-dataset and removing te zipped file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXaJJgJf1D2n"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Set the paths and filenames\n",
        "zip_file = '/content/flickr-image-dataset.zip'\n",
        "extracted_folder = '/content/IMAGE CAPTIONS'\n",
        " \n",
        "# Extract the contents of the zip file\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder)\n",
        "\n",
        "# Delete the zip file\n",
        "os.remove(zip_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gIkvHLM4d40"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d awsaf49/coco-2017-dataset \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KPkuUKzT6dvX"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "zip_path = '/content/coco-2017-dataset.zip'\n",
        "extract_path = '/content/IMAGE CAPTIONS'\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    # Extract the contents of the zip file with a progress bar\n",
        "    for file in tqdm(zip_ref.namelist(), desc='Extracting'):\n",
        "        zip_ref.extract(file, extract_path)\n",
        "\n",
        "# Remove the zipped file\n",
        "os.remove(zip_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xtA__SU9nIm"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Read the annotations file\n",
        "with open('/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json', 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Print the number of annotations\n",
        "print(\"Number of annotations:\", len(annotations['annotations']))\n",
        "\n",
        "# Extract image IDs and captions from the annotations\n",
        "image_ids = [ann['image_id'] for ann in annotations['annotations']]\n",
        "captions = [ann['caption'] for ann in annotations['annotations']]\n",
        "\n",
        "# Display a sample image ID and caption\n",
        "print(\"Sample image ID:\", image_ids[0])\n",
        "print(\"Sample caption:\", captions[0])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXIkVV7w-kjt"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Read the annotations file\n",
        "with open('/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Print the keys at the top level of the dataset\n",
        "print(\"Top-level keys:\", list(dataset.keys()))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdDLZL7z03r5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cqXZpq27-6l1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Read the annotations file\n",
        "with open('/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Access nested keys\n",
        "images = dataset['images']  # Access the 'images' key\n",
        "annotations = dataset['annotations']  # Access the 'annotations' key\n",
        "\n",
        "# Print the length of the images and annotations\n",
        "print(\"Number of images:\", len(images))\n",
        "print(\"Number of annotations:\", len(annotations))\n",
        "\n",
        "# Access further nested keys\n",
        "first_image = images[0]  # Access the first image\n",
        "image_id = first_image['id']  # Access the 'id' key of the first image\n",
        "\n",
        "first_annotation = annotations[0]  # Access the first annotation\n",
        "caption = first_annotation['caption']  # Access the 'caption' key of the first annotation\n",
        "\n",
        "# Print the image ID and caption\n",
        "print(\"Image ID:\", image_id)\n",
        "print(\"Caption:\", caption)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7JrlXvz_aMu"
      },
      "source": [
        "**# Check for missing values**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WRE8DADj_SFr"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Read the annotations file\n",
        "with open('/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json', 'r') as f:\n",
        "    dataset = json.load(f)\n",
        "\n",
        "# Check for missing values in images\n",
        "images = dataset['images']\n",
        "missing_images = sum(1 for image in images if not image['file_name'])\n",
        "print(\"Number of missing values in images:\", missing_images)\n",
        "\n",
        "# Check for missing values in annotations\n",
        "annotations = dataset['annotations']\n",
        "missing_annotations = sum(1 for annotation in annotations if not annotation['caption'])\n",
        "print(\"Number of missing values in annotations:\", missing_annotations)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHqBPpR6_i17"
      },
      "source": [
        "**# Visualize the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d7oIMJLE_lum"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "# Read an image\n",
        "image_path = '/content/IMAGE CAPTIONS/coco2017/train2017/000000005142.jpg'\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Read the corresponding caption\n",
        "caption = \"A person riding a wave on a surfboard.\"\n",
        "print(\"Caption:\", caption)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvXBalZbCGBP"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import json\n",
        "\n",
        "# Path to the image and annotation files\n",
        "image_path = '/content/IMAGE CAPTIONS/coco2017/train2017/000000005142.jpg'\n",
        "annotation_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "\n",
        "# Read the image\n",
        "image = Image.open(image_path)\n",
        "\n",
        "# Load the annotations file\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_dataset = json.load(f)\n",
        "\n",
        "# Get the corresponding annotation for the image\n",
        "annotations = coco_dataset['annotations']\n",
        "image_id = int(image_path.split('/')[-1].split('.')[0])\n",
        "image_annotations = [ann['caption'] for ann in annotations if ann['image_id'] == image_id]\n",
        "\n",
        "# Display the image with its annotation\n",
        "plt.imshow(image)\n",
        "plt.axis('off')\n",
        "plt.title('Image with Annotation')\n",
        "plt.suptitle('\\n'.join(image_annotations), fontsize=12, y=0.8)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcWVWL_bCxp9"
      },
      "source": [
        "**# Perform basic statistical analysis**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wGQpuJtIC2DW"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "\n",
        "# Load the annotations file\n",
        "annotation_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_dataset = json.load(f)\n",
        "\n",
        "# Get the image information\n",
        "images = coco_dataset['images']\n",
        "\n",
        "# Extract the widths and heights of the images\n",
        "widths = [img['width'] for img in images]\n",
        "heights = [img['height'] for img in images]\n",
        "\n",
        "# Perform basic statistical analysis\n",
        "width_stats = {\n",
        "    'Mean': np.mean(widths),\n",
        "    'Median': np.median(widths),\n",
        "    'Minimum': np.min(widths),\n",
        "    'Maximum': np.max(widths),\n",
        "    'Count': len(widths)\n",
        "}\n",
        "\n",
        "height_stats = {\n",
        "    'Mean': np.mean(heights),\n",
        "    'Median': np.median(heights),\n",
        "    'Minimum': np.min(heights),\n",
        "    'Maximum': np.max(heights),\n",
        "    'Count': len(heights)\n",
        "}\n",
        "\n",
        "# Print the statistics\n",
        "print('Image Width Statistics:')\n",
        "for stat, value in width_stats.items():\n",
        "    print(f'{stat}: {value}')\n",
        "print()\n",
        "print('Image Height Statistics:')\n",
        "for stat, value in height_stats.items():\n",
        "    print(f'{stat}: {value}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iotTdguDfet"
      },
      "source": [
        "**Available keys in the annotations dictionary:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZ0K0i4zCztw"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Load the annotations file\n",
        "annotation_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_dataset = json.load(f)\n",
        "\n",
        "# Get the first annotation dictionary\n",
        "annotation = coco_dataset['annotations'][0]\n",
        "\n",
        "# Display the available keys\n",
        "print(\"Available keys in the annotation dictionary:\")\n",
        "for key in annotation.keys():\n",
        "    print(key)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QONP61dVDyFp"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Load the annotations file\n",
        "annotation_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_dataset = json.load(f)\n",
        "\n",
        "# Get the annotations\n",
        "annotations = coco_dataset['annotations']\n",
        "\n",
        "# Extract the image IDs from the annotations\n",
        "image_ids = [ann['image_id'] for ann in annotations]\n",
        "\n",
        "# Get the image information for the corresponding image IDs\n",
        "images = coco_dataset['images']\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTD8gsQ7JpXY"
      },
      "source": [
        "**DO NOT EXECUTE THE CODE but it reads the annotations file for the COCO dataset and extracts the categories for each image. It then counts the occurrences of each category and prints the most common categories**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CksSkryAIFl2"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "from collections import Counter\n",
        "\n",
        "# Load the annotations file\n",
        "annotation_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "with open(annotation_file, 'r') as f:\n",
        "    coco_dataset = json.load(f)\n",
        "\n",
        "# Get the annotations\n",
        "annotations = coco_dataset['annotations']\n",
        "\n",
        "# Extract the image IDs from the annotations\n",
        "image_ids = [ann['image_id'] for ann in annotations]\n",
        "\n",
        "# Get the image information for the corresponding image IDs\n",
        "images = coco_dataset['images']\n",
        "\n",
        "# Extract the categories for each image\n",
        "image_categories = []\n",
        "for img in images:\n",
        "    if img['id'] in image_ids:\n",
        "        image_annotations = [ann for ann in annotations if ann['image_id'] == img['id']]\n",
        "        category_ids = set()\n",
        "        for ann in image_annotations:\n",
        "            if 'category_ids' in ann:\n",
        "                image_category_ids = [cat_id for cat_id in ann['category_ids']]\n",
        "                category_ids.update(image_category_ids)\n",
        "        category_names = [coco_dataset['categories'][cat_id]['name'] for cat_id in category_ids]\n",
        "        image_categories.extend(category_names)\n",
        "\n",
        "# Count the occurrences of each category\n",
        "category_counts = Counter(image_categories)\n",
        "\n",
        "# Print the most common categories\n",
        "print('Most Common Categories:')\n",
        "for category, count in category_counts.most_common(10):  # Change the number to display more or fewer categories\n",
        "    print(f'{category}: {count}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc0onvl7Lf60"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "\n",
        "```\n",
        "\n",
        "# DATA PROCEESSING AND ANALYSING"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4NzaeFePHHh"
      },
      "source": [
        "**Load the image data and corresponding captions from the dataset.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2YnvdlxL48V"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "# Path to the annotations file\n",
        "annotations_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "\n",
        "# Read the annotations file\n",
        "with open(annotations_file, 'r') as f:\n",
        "    annotations = json.load(f)\n",
        "\n",
        "# Extract the image information and captions\n",
        "images = annotations['images']\n",
        "captions = annotations['annotations']\n",
        "\n",
        "# Create a dictionary to map image IDs to their corresponding file names\n",
        "image_id_to_filename = {img['id']: img['file_name'] for img in images}\n",
        "\n",
        "# Create a list to store the image paths and corresponding captions\n",
        "image_paths = []\n",
        "image_captions = []\n",
        "\n",
        "# Loop through the captions and append the image paths and captions to the lists\n",
        "for caption in captions:\n",
        "    image_id = caption['image_id']\n",
        "    caption_text = caption['caption']\n",
        "    image_filename = image_id_to_filename[image_id]\n",
        "    image_path = os.path.join('/content/IMAGE CAPTIONS/coco2017/train2017', image_filename)\n",
        "    image_paths.append(image_path)\n",
        "    image_captions.append(caption_text)\n",
        "\n",
        "# Print the total number of images and captions\n",
        "print(\"Total number of images:\", len(image_paths))\n",
        "print(\"Total number of captions:\", len(image_captions))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbfUfXGdPXM-"
      },
      "source": [
        "**Resizing the images, Normalizing the pixel values and Converting the images to arrays:(IMAGE PROCESSSING)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ta26uS6WPqSx"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Preprocessing parameters\n",
        "target_size = (224, 224)\n",
        "max_pixel_value = 255.0\n",
        "\n",
        "# Preprocess the images\n",
        "preprocessed_images = []\n",
        "for image_path in tqdm(image_paths, desc=\"Preprocessing images\", unit=\"image\"):\n",
        "    # Load the image\n",
        "    image = Image.open(image_path)\n",
        "    \n",
        "    # Resize the image\n",
        "    image = image.resize(target_size)\n",
        "    \n",
        "    # Convert the image to array\n",
        "    image_array = np.array(image)\n",
        "    \n",
        "    # Normalize the pixel values\n",
        "    image_array = image_array / max_pixel_value\n",
        "    \n",
        "    # Append the preprocessed image to the list\n",
        "    preprocessed_images.append(image_array)\n",
        "\n",
        "# Convert the list of preprocessed images to a numpy array\n",
        "preprocessed_images = np.array(preprocessed_images)\n",
        "\n",
        "# Print the shape of the preprocessed images array\n",
        "print(\"Preprocessed images shape:\", preprocessed_images.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Set the path to the JSON file\n",
        "json_path = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "\n",
        "# Load the JSON file\n",
        "with open(json_path, 'r') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Extract the captions from the JSON data\n",
        "captions = []\n",
        "for item in data['annotations']:\n",
        "    caption = item['caption']\n",
        "    captions.append(caption)\n",
        "\n",
        "# Create a DataFrame with the captions\n",
        "captions_df = pd.DataFrame({'caption': captions})\n"
      ],
      "metadata": {
        "id": "3mjcZAyDsHql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Print out of JSON files in a annotations directory.** "
      ],
      "metadata": {
        "id": "X_wQZtkJvvTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "directory = '/content/IMAGE CAPTIONS/coco2017/annotations'\n",
        "\n",
        "# Get the list of JSON files in the directory\n",
        "files = [file for file in os.listdir(directory) if file.endswith('.json')]\n",
        "\n",
        "# Print the keys of JSON files\n",
        "for file in files:\n",
        "    file_path = os.path.join(directory, file)\n",
        "    with open(file_path, 'r') as f:\n",
        "        data = json.load(f)\n",
        "        print(f\"--- {file} ---\")\n",
        "        print(data.keys())\n",
        "        print()\n"
      ],
      "metadata": {
        "id": "_DshGUcEu2dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA CLEANING ON JSON FILES USING ANNOTATIONS FILE**"
      ],
      "metadata": {
        "id": "ew9Hl5VXwO_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "\n",
        "# Set the paths to the JSON files\n",
        "annotations_dir = '/content/IMAGE CAPTIONS/coco2017/annotations'\n",
        "captions_train_file = os.path.join(annotations_dir, 'captions_train2017.json')\n",
        "\n",
        "# Read the captions from the JSON file\n",
        "with open(captions_train_file, 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Process and clean the captions\n",
        "captions = captions_data['annotations']\n",
        "captions_cleaned = []\n",
        "\n",
        "for caption in captions:\n",
        "    # Remove special characters and punctuation\n",
        "    caption_text = re.sub(r\"[^\\w\\s]\", \"\", caption['caption'])\n",
        "    # Remove extra whitespace\n",
        "    caption_text = re.sub(r\"\\s+\", \" \", caption_text)\n",
        "    # Convert to lowercase\n",
        "    caption_text = caption_text.lower()\n",
        "\n",
        "    # Filter out short or long captions\n",
        "    if len(caption_text.split()) >= 5 and len(caption_text.split()) <= 20:\n",
        "        captions_cleaned.append(caption_text)\n",
        "\n",
        "# Remove duplicate captions\n",
        "unique_captions = list(set(captions_cleaned))\n",
        "\n",
        "# Print a limited number of cleaned captions\n",
        "num_captions_to_print = 10\n",
        "\n",
        "for i in range(num_captions_to_print):\n",
        "    print(unique_captions[i])\n",
        "\n"
      ],
      "metadata": {
        "id": "h74JyjhawPty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FURTHER CLEANING**"
      ],
      "metadata": {
        "id": "bLeXx9Aw0D18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Set the paths to the JSON files\n",
        "annotations_dir = '/content/IMAGE CAPTIONS/coco2017/annotations'\n",
        "captions_train_file = os.path.join(annotations_dir, 'captions_train2017.json')\n",
        "\n",
        "# Read the captions from the JSON file\n",
        "with open(captions_train_file, 'r') as f:\n",
        "    captions_data = json.load(f)\n",
        "\n",
        "# Process and clean the captions\n",
        "captions = captions_data['annotations']\n",
        "captions_cleaned = []\n",
        "\n",
        "# Define the set of stopwords to be removed\n",
        "stopwords = set(['a', 'an', 'the', 'is', 'are', 'was', 'were', 'in', 'on', 'at', 'to', 'for', 'of'])\n",
        "\n",
        "for caption in captions:\n",
        "    # Remove special characters and punctuation\n",
        "    caption_text = re.sub(r\"[^\\w\\s]\", \"\", caption['caption'])\n",
        "    # Remove extra whitespace\n",
        "    caption_text = re.sub(r\"\\s+\", \" \", caption_text)\n",
        "    # Convert to lowercase\n",
        "    caption_text = caption_text.lower()\n",
        "\n",
        "    # Filter out short or long captions\n",
        "    if len(caption_text.split()) >= 5 and len(caption_text.split()) <= 20:\n",
        "        # Remove stopwords\n",
        "        caption_words = caption_text.split()\n",
        "        caption_words = [word for word in caption_words if word not in stopwords]\n",
        "        # Join the words back into a caption\n",
        "        caption_cleaned = \" \".join(caption_words)\n",
        "        captions_cleaned.append(caption_cleaned)\n",
        "\n",
        "# Remove duplicate captions\n",
        "unique_captions = list(set(captions_cleaned))\n",
        "\n",
        "# Filter out captions with low frequency\n",
        "caption_frequency = Counter(captions_cleaned)\n",
        "min_frequency = 5\n",
        "filtered_captions = [caption for caption in captions_cleaned if caption_frequency[caption] >= min_frequency]\n",
        "\n",
        "# Print a limited number of filtered captions\n",
        "num_captions_to_print = 10\n",
        "\n",
        "for i in range(num_captions_to_print):\n",
        "    print(filtered_captions[i])\n"
      ],
      "metadata": {
        "id": "_-ruPudt0Mdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DATA PROCESSING, including image preprocessing and caption tokenization:**"
      ],
      "metadata": {
        "id": "VsrTtl5p0S0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the paths for the MSCOCO dataset\n",
        "dataset_path = '/content/IMAGE CAPTIONS/coco2017'\n",
        "images_directory = os.path.join(dataset_path, 'train2017')\n",
        "captions_file = os.path.join(dataset_path, 'annotations', 'captions_train2017.json')\n",
        "\n",
        "# Set the maximum number of words in the vocabulary\n",
        "max_vocab_size = 10000\n",
        "\n",
        "# Set the maximum length of captions (in words)\n",
        "max_caption_length = 20\n",
        "\n",
        "# Set the target image size\n",
        "target_image_size = (256, 256)\n",
        "\n",
        "# Process the images\n",
        "def process_images(images_directory):\n",
        "    processed_images = []\n",
        "    for filename in os.listdir(images_directory):\n",
        "        image_path = os.path.join(images_directory, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.resize(image, target_image_size)\n",
        "        processed_images.append(image)\n",
        "    return np.array(processed_images)\n",
        "\n",
        "# Process the captions\n",
        "def process_captions(captions_file):\n",
        "    with open(captions_file, 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "    \n",
        "    captions = []\n",
        "    for annotation in captions_data['annotations']:\n",
        "        captions.append(annotation['caption'])\n",
        "    \n",
        "    # Tokenize the captions\n",
        "    tokenizer = Tokenizer(num_words=max_vocab_size, oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(captions)\n",
        "    captions_sequences = tokenizer.texts_to_sequences(captions)\n",
        "    \n",
        "    # Pad the sequences\n",
        "    captions_padded = pad_sequences(captions_sequences, maxlen=max_caption_length, padding='post')\n",
        "    \n",
        "    return captions_padded, tokenizer\n",
        "\n",
        "# Preprocess the images\n",
        "processed_images = process_images(images_directory)\n",
        "\n",
        "# Preprocess the captions\n",
        "captions_padded, tokenizer = process_captions(captions_file)\n",
        "\n",
        "# Save the preprocessed data\n",
        "np.save('processed_images.npy', processed_images)\n",
        "np.save('captions_padded.npy', captions_padded)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_json = tokenizer.to_json()\n",
        "with open('tokenizer.json', 'w') as f:\n",
        "    f.write(tokenizer_json)\n"
      ],
      "metadata": {
        "id": "VxbBtU1WFmA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Image Preprocessing, Caption Loading, Sequencing and Tokenization:**"
      ],
      "metadata": {
        "id": "0Je4p5vJStl9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "# Set the path to the image directory\n",
        "image_dir = '/content/IMAGE CAPTIONS/coco2017/train2017'\n",
        "\n",
        "# Set the path to the caption file\n",
        "caption_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "\n",
        "# Set the desired image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Set the maximum number of words in the vocabulary\n",
        "max_vocab_size = 10000\n",
        "\n",
        "# Set the maximum length of the caption sequences\n",
        "max_caption_length = 50\n",
        "\n",
        "# Define the image data generator for preprocessing\n",
        "image_data_generator = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0  # Normalize pixel values between 0 and 1\n",
        ")\n",
        "\n",
        "# Define the function to load captions from the caption file\n",
        "def load_captions(caption_file):\n",
        "    with open(caption_file, 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "    captions = []\n",
        "    for annotation in captions_data['annotations']:\n",
        "        captions.append(annotation['caption'])\n",
        "    return captions\n",
        "\n",
        "# Define the function to preprocess the images\n",
        "def preprocess_images(image_paths):\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        image = cv2.imread(os.path.join(image_dir, image_path))\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Define the data generator class for batch processing\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, image_dir, caption_file, batch_size, image_data_generator):\n",
        "        self.image_dir = image_dir\n",
        "        self.caption_file = caption_file\n",
        "        self.batch_size = batch_size\n",
        "        self.image_data_generator = image_data_generator\n",
        "\n",
        "        # Load the captions and image filenames\n",
        "        self.captions = load_captions(caption_file)\n",
        "        self.image_filenames = sorted(os.listdir(image_dir))\n",
        "\n",
        "        # Tokenize the captions and create the vocabulary\n",
        "        self.tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<unk>\")\n",
        "        self.tokenizer.fit_on_texts(self.captions)\n",
        "        self.word_index = self.tokenizer.word_index\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_image_filenames = self.image_filenames[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_captions = self.captions[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = preprocess_images(batch_image_filenames)\n",
        "\n",
        "        # Tokenize the captions and convert them into sequences of tokens\n",
        "        batch_tokenized_captions = self.tokenizer.texts_to_sequences(batch_captions)\n",
        "\n",
        "        # Pad the caption sequences to a fixed length\n",
        "        batch_padded_captions = pad_sequences(batch_tokenized_captions, maxlen=max_caption_length, padding='post')\n",
        "\n",
        "        return batch_images, batch_padded_captions\n",
        "\n",
        "# Set the batch size and create data generators for training, validation, and test\n",
        "batch_size = 32\n",
        "train_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/train2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json\", batch_size, image_data_generator)\n",
        "val_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/val2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_val2017.json\", batch_size, image_data_generator)\n",
        "\n",
        "# Print the vocabulary size\n",
        "vocab_size = min(len(train_generator.word_index) + 1, max_vocab_size)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n",
        "# Print the number of training, validation, and test samples\n",
        "print(\"Number of training samples:\", len(train_generator.image_filenames))\n",
        "print(\"Number of validation samples:\", len(val_generator.image_filenames))\n"
      ],
      "metadata": {
        "id": "2otmH_5dWZoe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "H0VwxdSuTZtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "# Set the path to the image directory\n",
        "image_dir = '/content/IMAGE CAPTIONS/coco2017/train2017'\n",
        "\n",
        "# Set the path to the caption file\n",
        "caption_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "\n",
        "# Set the desired image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Set the maximum number of words in the vocabulary\n",
        "max_vocab_size = 10000\n",
        "\n",
        "# Set the maximum length of the caption sequences\n",
        "max_caption_length = 50\n",
        "\n",
        "# Define the image data generator for preprocessing\n",
        "image_data_generator = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0  # Normalize pixel values between 0 and 1\n",
        ")\n",
        "\n",
        "# Define the function to load captions from the caption file\n",
        "def load_captions(caption_file):\n",
        "    with open(caption_file, 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "    captions = []\n",
        "    for annotation in captions_data['annotations']:\n",
        "        captions.append(annotation['caption'])\n",
        "    return captions\n",
        "\n",
        "# Define the function to preprocess the images\n",
        "def preprocess_images(image_paths):\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        image = cv2.imread(os.path.join(image_dir, image_path))\n",
        "        if image is None:\n",
        "            print(f\"Failed to load image: {image_path}\")\n",
        "            continue\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Define the function to load an image from the image file\n",
        "def load_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        return image\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Define the data generator class for batch processing\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, image_dir, caption_file, batch_size, image_data_generator):\n",
        "        self.image_dir = image_dir\n",
        "        self.caption_file = caption_file\n",
        "        self.batch_size = batch_size\n",
        "        self.image_data_generator = image_data_generator\n",
        "\n",
        "        # Load the captions and image filenames\n",
        "        self.captions = load_captions(caption_file)\n",
        "        self.image_filenames = sorted(os.listdir(image_dir))\n",
        "        self.image_filenames = self.filter_images(self.image_filenames)\n",
        "\n",
        "        # Tokenize the captions and create the vocabulary\n",
        "        self.tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<unk>\")\n",
        "        self.tokenizer.fit_on_texts(self.captions)\n",
        "        self.word_index = self.tokenizer.word_index\n",
        "\n",
        "    def filter_images(self, image_filenames):\n",
        "        filtered_image_filenames = []\n",
        "        for image_filename in image_filenames:\n",
        "            image_path = os.path.join(self.image_dir, image_filename)\n",
        "            if load_image(image_path) is not None:\n",
        "                filtered_image_filenames.append(image_filename)\n",
        "            else:\n",
        "                print(f\"Failed to load image: {image_filename}\")\n",
        "        return filtered_image_filenames\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_image_filenames = self.image_filenames[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_captions = self.captions[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = preprocess_images(batch_image_filenames)\n",
        "\n",
        "        # Tokenize the captions and convert them into sequences of tokens\n",
        "        batch_tokenized_captions = self.tokenizer.texts_to_sequences(batch_captions)\n",
        "\n",
        "        # Pad the caption sequences to a fixed length\n",
        "        batch_padded_captions = pad_sequences(batch_tokenized_captions, maxlen=max_caption_length, padding='post')\n",
        "\n",
        "        return batch_images, batch_padded_captions\n",
        "\n",
        "        # Tokenize the captions and convert them into sequences of tokens\n",
        "        batch_tokenized_captions = self.tokenizer.texts_to_sequences(batch_captions)\n",
        "\n",
        "        # Pad the caption sequences to a fixed length\n",
        "        batch_padded_captions = pad_sequences(batch_tokenized_captions, maxlen=max_caption_length, padding='post')\n",
        "\n",
        "        return batch_images, batch_padded_captions\n",
        "\n",
        "# Set the batch size and create data generators for training, validation, and test\n",
        "batch_size = 32\n",
        "train_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/train2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json\", batch_size, image_data_generator)\n",
        "val_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/val2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_val2017.json\", batch_size, image_data_generator)\n",
        "\n",
        "# Set the path to the directory for processed information\n",
        "processed_dir = '/content/processed_data'\n",
        "os.makedirs(processed_dir, exist_ok=True)\n",
        "\n",
        "# Save the processed information\n",
        "np.save(os.path.join(processed_dir, 'train_images.npy'), train_generator[0][0])\n",
        "np.save(os.path.join(processed_dir, 'train_captions.npy'), train_generator[0][1])\n",
        "np.save(os.path.join(processed_dir, 'val_images.npy'), val_generator[0][0])\n",
        "np.save(os.path.join(processed_dir, 'val_captions.npy'), val_generator[0][1])\n",
        "\n",
        "# Save the word index\n",
        "with open(os.path.join(processed_dir, 'word_index.json'), 'w') as f:\n",
        "    json.dump(train_generator.word_index, f)\n",
        "\n",
        "# Print the vocabulary size\n",
        "vocab_size = min(len(train_generator.word_index) + 1, max_vocab_size)\n",
        "print(\"Vocabulary size:\", vocab_size)\n",
        "\n"
      ],
      "metadata": {
        "id": "5HSNLGgFZRe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#***`MODEL TRAINING `***"
      ],
      "metadata": {
        "id": "4k2P0gL5d0sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set the path to the image directory\n",
        "image_dir = '/content/IMAGE CAPTIONS/coco2017/train2017'\n",
        "\n",
        "# Set the path to the caption file\n",
        "caption_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "\n",
        "# Set the desired image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Set the maximum number of words in the vocabulary\n",
        "max_vocab_size = 10000\n",
        "\n",
        "# Set the maximum length of the caption sequences\n",
        "max_caption_length = 50\n",
        "\n",
        "# Define the image data generator for preprocessing\n",
        "image_data_generator = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0  # Normalize pixel values between 0 and 1\n",
        ")\n",
        "\n",
        "# Define the function to load captions from the caption file\n",
        "def load_captions(caption_file):\n",
        "    with open(caption_file, 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "    captions = []\n",
        "    for annotation in captions_data['annotations']:\n",
        "        captions.append(annotation['caption'])\n",
        "    return captions\n",
        "\n",
        "# Define the function to preprocess the images\n",
        "def preprocess_images(image_paths):\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        image = cv2.imread(os.path.join(image_dir, image_path))\n",
        "        if image is None:\n",
        "            print(f\"Failed to load image: {image_path}\")\n",
        "            continue\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Define the function to load an image from the image file\n",
        "def load_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        return image\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Define the data generator class for batch processing\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, image_dir, caption_file, batch_size, image_data_generator):\n",
        "        self.image_dir = image_dir\n",
        "        self.caption_file = caption_file\n",
        "        self.batch_size = batch_size\n",
        "        self.image_data_generator = image_data_generator\n",
        "\n",
        "        # Load the captions and image filenames\n",
        "        self.captions = load_captions(caption_file)\n",
        "        self.image_filenames = get_image_filenames(image_dir)\n",
        "        self.image_filenames = self.filter_images(self.image_filenames)\n",
        "\n",
        "        # Tokenize the captions and create the vocabulary\n",
        "        self.tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<unk>\")\n",
        "        self.tokenizer.fit_on_texts(self.captions)\n",
        "        self.word_index = self.tokenizer.word_index\n",
        "\n",
        "    # Rest of the code...\n",
        "\n",
        "    def filter_images(self, image_filenames):\n",
        "        filtered_image_filenames = []\n",
        "        for image_filename in image_filenames:\n",
        "            image_path = os.path.join(self.image_dir, image_filename)\n",
        "            if load_image(image_path) is not None:\n",
        "                filtered_image_filenames.append(image_filename)\n",
        "            else:\n",
        "                print(f\"Failed to load image: {image_filename}\")\n",
        "        return filtered_image_filenames\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_image_filenames = self.image_filenames[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_captions = self.captions[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = preprocess_images(batch_image_filenames)\n",
        "\n",
        "        # Tokenize the captions and convert them into sequences of tokens\n",
        "        batch_tokenized_captions = self.tokenizer.texts_to_sequences(batch_captions)\n",
        "\n",
        "        # Pad the caption sequences to a fixed length\n",
        "        batch_padded_captions = pad_sequences(batch_tokenized_captions, maxlen=max_caption_length, padding='post')\n",
        "\n",
        "        return batch_images, batch_padded_captions\n",
        "\n",
        "# Set the batch size and create data generators for training, validation, and test\n",
        "batch_size = 32\n",
        "train_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/train2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json\", batch_size, image_data_generator)\n",
        "val_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/val2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_val2017.json\", batch_size, image_data_generator)\n",
        "\n",
        "# Define the encoder architecture using MobileNetV2\n",
        "input_shape = image_size + (3,)  # Image shape with 3 channels (RGB)\n",
        "base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze the weights of the pre-trained model\n",
        "encoder_output = base_model.output\n",
        "encoder_output = Flatten()(encoder_output)\n",
        "encoder = Model(inputs=base_model.input, outputs=encoder_output)\n",
        "\n",
        "# Define the decoder architecture using LSTM\n",
        "embedding_dim = 256\n",
        "decoder_units = 256\n",
        "\n",
        "# Define the input for the decoder\n",
        "decoder_input = Input(shape=(max_caption_length,))\n",
        "\n",
        "# Define the embedding layer\n",
        "embedding_layer = Embedding(input_dim=max_vocab_size, output_dim=embedding_dim)(decoder_input)\n",
        "\n",
        "# Define the LSTM layer\n",
        "decoder_lstm = LSTM(units=decoder_units, return_sequences=True)(embedding_layer)\n",
        "\n",
        "# Define the output layer\n",
        "decoder_output = Dense(units=max_vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "# Define the decoder model\n",
        "decoder = Model(inputs=decoder_input, outputs=decoder_output)\n",
        "\n",
        "# Define the complete model by connecting the encoder and decoder\n",
        "image_input = Input(shape=input_shape)\n",
        "encoded_image = encoder(image_input)\n",
        "decoded_caption = decoder(encoded_image)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=image_input, outputs=decoded_caption)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer_path = \"/content/tokenizer.pkl\"\n",
        "with open(tokenizer_path, \"wb\") as f:\n",
        "    pickle.dump(train_generator.tokenizer, f)\n",
        "\n",
        "# Save the encoder model\n",
        "encoder_model_path = \"/content/encoder_model.h5\"\n",
        "encoder.save(encoder_model_path)\n",
        "\n",
        "# Save the decoder model\n",
        "decoder_model_path = \"/content/decoder_model.h5\"\n",
        "decoder.save(decoder_model_path)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Fit the model using the data generators\n",
        "model.fit(train_generator, epochs=epochs, validation_data=val_generator, verbose=1, callbacks=[tqdm])\n",
        "\n",
        "# Save the trained model\n",
        "model_path = \"/content/trained_model.h5\"\n",
        "model.save(model_path)\n"
      ],
      "metadata": {
        "id": "cFTt03EDUP4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "import cv2\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tqdm import tqdm  # Import tqdm library for progress bar\n",
        "\n",
        "# Set the path to the image directory\n",
        "image_dir = '/content/IMAGE CAPTIONS/coco2017/train2017'\n",
        "\n",
        "# Set the path to the caption file\n",
        "caption_file = '/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json'\n",
        "\n",
        "# Set the desired image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Set the maximum number of words in the vocabulary\n",
        "max_vocab_size = 10000\n",
        "\n",
        "# Set the maximum length of the caption sequences\n",
        "max_caption_length = 50\n",
        "\n",
        "# Define the image data generator for preprocessing\n",
        "image_data_generator = ImageDataGenerator(\n",
        "    rescale=1.0 / 255.0  # Normalize pixel values between 0 and 1\n",
        ")\n",
        "\n",
        "# Define the function to load captions from the caption file\n",
        "def load_captions(caption_file):\n",
        "    with open(caption_file, 'r') as f:\n",
        "        captions_data = json.load(f)\n",
        "    captions = []\n",
        "    for annotation in captions_data['annotations']:\n",
        "        captions.append(annotation['caption'])\n",
        "    return captions\n",
        "\n",
        "# Define the function to preprocess the images\n",
        "def preprocess_images(image_paths):\n",
        "    images = []\n",
        "    for image_path in image_paths:\n",
        "        image = cv2.imread(os.path.join(image_dir, image_path))\n",
        "        if image is None:\n",
        "            print(f\"Failed to load image: {image_path}\")\n",
        "            continue\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Define the function to load an image from the image file\n",
        "def load_image(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is not None:\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        return image\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Define the data generator class for batch processing\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, image_dir, caption_file, batch_size, image_data_generator):\n",
        "        self.image_dir = image_dir\n",
        "        self.caption_file = caption_file\n",
        "        self.batch_size = batch_size\n",
        "        self.image_data_generator = image_data_generator\n",
        "\n",
        "\n",
        "import os\n",
        "\n",
        "# Define the function to get image filenames from the image directory\n",
        "def get_image_filenames(image_dir):\n",
        "    image_filenames = os.listdir(image_dir)\n",
        "    return image_filenames\n",
        "\n",
        "# Define the data generator class for batch processing\n",
        "class DataGenerator(Sequence):\n",
        "    def __init__(self, image_dir, caption_file, batch_size, image_data_generator):\n",
        "        self.image_dir = image_dir\n",
        "        self.caption_file = caption_file\n",
        "        self.batch_size = batch_size\n",
        "        self.image_data_generator = image_data_generator\n",
        "\n",
        "        # Load the captions and image filenames\n",
        "        self.captions = load_captions(caption_file)\n",
        "        self.image_filenames = get_image_filenames(image_dir)\n",
        "        self.image_filenames = self.filter_images(self.image_filenames)\n",
        "\n",
        "        # Tokenize the captions and create the vocabulary\n",
        "        self.tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<unk>\")\n",
        "        self.tokenizer.fit_on_texts(self.captions)\n",
        "        self.word_index = self.tokenizer.word_index\n",
        "\n",
        "    # Rest of the code...\n",
        "\n",
        "    def filter_images(self, image_filenames):\n",
        "        filtered_image_filenames = []\n",
        "        for image_filename in image_filenames:\n",
        "            image_path = os.path.join(self.image_dir, image_filename)\n",
        "            if load_image(image_path) is not None:\n",
        "                filtered_image_filenames.append(image_filename)\n",
        "            else:\n",
        "                print(f\"Failed to load image: {image_filename}\")\n",
        "        return filtered_image_filenames\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.image_filenames) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        batch_image_filenames = self.image_filenames[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "        batch_captions = self.captions[index * self.batch_size: (index + 1) * self.batch_size]\n",
        "\n",
        "        batch_images = preprocess_images(batch_image_filenames)\n",
        "\n",
        "        # Tokenize the captions and convert them into sequences of tokens\n",
        "        batch_tokenized_captions = self.tokenizer.texts_to_sequences(batch_captions)\n",
        "\n",
        "        # Pad the caption sequences to a fixed length\n",
        "        batch_padded_captions = pad_sequences(batch_tokenized_captions, maxlen=max_caption_length, padding='post')\n",
        "\n",
        "        return batch_images, batch_padded_captions\n",
        "\n",
        "# Set the batch size and create data generators for training, validation, and test\n",
        "batch_size = 32\n",
        "train_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/train2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_train2017.json\", batch_size, image_data_generator)\n",
        "val_generator = DataGenerator(\"/content/IMAGE CAPTIONS/coco2017/val2017\", \"/content/IMAGE CAPTIONS/coco2017/annotations/captions_val2017.json\", batch_size, image_data_generator)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "l_6DpVzKyBWh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Continuation**"
      ],
      "metadata": {
        "id": "2-ix_1ckJSyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the desired image size\n",
        "image_size = (224, 224)\n",
        "# Define the encoder architecture using MobileNetV2\n",
        "input_shape = image_size + (3,)  # Image shape with 3 channels (RGB)\n",
        "base_model = MobileNetV2(input_shape=input_shape, include_top=False, weights='imagenet')\n",
        "base_model.trainable = False  # Freeze the weights of the pre-trained model\n",
        "encoder_output = base_model.output\n",
        "encoder_output = Flatten()(encoder_output)\n",
        "encoder = Model(inputs=base_model.input, outputs=encoder_output)\n",
        "\n",
        "# Define the decoder architecture using LSTM\n",
        "embedding_dim = 256\n",
        "decoder_units = 256\n",
        "\n",
        "# Define the input for the decoder\n",
        "decoder_input = Input(shape=(max_caption_length,))\n",
        "\n",
        "# Define the embedding layer\n",
        "embedding_layer = Embedding(input_dim=max_vocab_size, output_dim=embedding_dim)(decoder_input)\n",
        "\n",
        "# Define the LSTM layer\n",
        "decoder_lstm = LSTM(units=decoder_units, return_sequences=True)(embedding_layer)\n",
        "\n",
        "# Define the output layer\n",
        "decoder_output = Dense(units=max_vocab_size, activation='softmax')(decoder_lstm)\n",
        "\n",
        "# Define the decoder model\n",
        "decoder = Model(inputs=decoder_input, outputs=decoder_output)\n",
        "\n",
        "# Define the complete model by connecting the encoder and decoder\n",
        "image_input = Input(shape=input_shape)\n",
        "encoded_image = encoder(image_input)\n",
        "decoded_caption = decoder(encoded_image)\n",
        "\n",
        "# Compile the model\n",
        "model = Model(inputs=image_input, outputs=decoded_caption)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "from tensorflow.keras.callbacks import ProgbarLogger\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = 10\n",
        "\n",
        "# Fit the model using the data generators\n",
        "model.fit(train_generator, epochs=epochs, validation_data=val_generator, callbacks=[ProgbarLogger()])"
      ],
      "metadata": {
        "id": "BvzA1t_WJQI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ulxdtxRdueD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CAPTIONS GENERATOR**"
      ],
      "metadata": {
        "id": "jkK8FU9jEe-Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CaptionsGenerator:\n",
        "    def __init__(self, model, tokenizer, max_caption_length):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_caption_length = max_caption_length\n",
        "\n",
        "    def generate_caption(self, image):\n",
        "        # Preprocess the image\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "\n",
        "        # Generate the caption\n",
        "        initial_state = self.model.get_initial_state(image)\n",
        "        input_sequence = [self.tokenizer.word_index['<start>']]\n",
        "\n",
        "        while True:\n",
        "            inputs = {\n",
        "                'image': image,\n",
        "                'input_sequence': np.array([input_sequence]),\n",
        "                'initial_state': initial_state\n",
        "            }\n",
        "            outputs = self.model.predict(inputs)\n",
        "            predicted_id = np.argmax(outputs['output_token_probs'][0, -1, :])\n",
        "\n",
        "            # Exit the loop if the maximum caption length is reached or if the end token is predicted\n",
        "            if len(input_sequence) >= self.max_caption_length or predicted_id == self.tokenizer.word_index['<end>']:\n",
        "                break\n",
        "\n",
        "            input_sequence.append(predicted_id)\n",
        "\n",
        "        caption = self.tokenizer.sequences_to_texts([input_sequence])[0]\n",
        "        return caption\n"
      ],
      "metadata": {
        "id": "kd4h2AEYht0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = load_model('image_captioning_model.h5')\n",
        "\n",
        "# Load the word index\n",
        "with open(os.path.join(processed_dir, 'word_index.json'), 'r') as f:\n",
        "    word_index = json.load(f)\n",
        "\n",
        "# Reverse the word index to map token indices to words\n",
        "index_to_word = {index: word for word, index in word_index.items()}\n",
        "\n",
        "# Load the test images\n",
        "test_images = np.load(os.path.join(processed_dir, 'val_images.npy'))\n",
        "\n",
        "# Generate captions for the test images\n",
        "for image in test_images:\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    initial_caption = np.array([[word_index['<start>']]])  # Start token\n",
        "    end_token = word_index['<end>']  # End token\n",
        "\n",
        "    # Generate captions word by word\n",
        "    while True:\n",
        "        predictions = model.predict([image, initial_caption])\n",
        "        predicted_token_index = np.argmax(predictions[0, -1, :])\n",
        "        predicted_word = index_to_word[predicted_token_index]\n",
        "        initial_caption = np.append(initial_caption, [[predicted_token_index]], axis=1)\n",
        "\n",
        "        # Break if end token is generated or maximum caption length is reached\n",
        "        if predicted_token_index == end_token or initial_caption.shape[1] >= max_caption_length:\n",
        "            break\n",
        "\n",
        "    # Remove start and end tokens and convert token indices to words\n",
        "    generated_caption = initial_caption[0, 1:-1]\n",
        "    generated_caption_words = [index_to_word[token_index] for token_index in generated_caption]\n",
        "\n",
        "    # Print the generated caption\n",
        "    caption = ' '.join(generated_caption_words)\n",
        "    print(\"Generated Caption:\", caption)\n"
      ],
      "metadata": {
        "id": "u_aPAoLtfjBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **QUALITY EVALUTION**"
      ],
      "metadata": {
        "id": "NUUo4E3ghbO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "from pycocoevalcap.cider import Cider\n",
        "\n",
        "# Create a CaptionsGenerator instance for evaluation\n",
        "eval_captions_generator = CaptionsGenerator(model, tokenizer, max_caption_length)\n",
        "\n",
        "# Generate captions for the validation dataset\n",
        "val_captions = []\n",
        "generated_captions = []\n",
        "\n",
        "for i in tqdm(range(len(val_generator))):\n",
        "    batch_images, batch_captions = val_generator[i]\n",
        "\n",
        "    # Generate captions for each image in the batch\n",
        "    for j in range(batch_images.shape[0]):\n",
        "        image = batch_images[j]\n",
        "        caption = eval_captions_generator.generate_caption(image)\n",
        "\n",
        "        # Convert the ground truth caption from tokenized form to text\n",
        "        ground_truth_caption = tokenizer.sequences_to_texts([batch_captions[j]])[0]\n",
        "\n",
        "        val_captions.append([ground_truth_caption.split()])\n",
        "        generated_captions.append(caption.split())\n",
        "\n",
        "# Calculate the BLEU score\n",
        "bleu_score = corpus_bleu(val_captions, generated_captions)\n",
        "\n",
        "# Calculate the METEOR score\n",
        "meteor_score = meteor_score(val_captions, generated_captions)\n",
        "\n",
        "# Calculate the CIDEr score\n",
        "cider_scorer = Cider()\n",
        "cider_score, _ = cider_scorer.compute_score(val_captions, generated_captions)\n",
        "\n",
        "print(f\"BLEU score: {bleu_score}\")\n",
        "print(f\"METEOR score: {meteor_score}\")\n",
        "print(f\"CIDEr score: {cider_score}\")\n"
      ],
      "metadata": {
        "id": "OO0wNpY0ilJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "second alternative"
      ],
      "metadata": {
        "id": "KDkgSUcdim7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "# Load the reference captions\n",
        "reference_captions = np.load(os.path.join(processed_dir, 'val_captions.npy'))\n",
        "\n",
        "# Prepare the reference captions in the required format\n",
        "reference_captions = [[caption.split()] for caption in reference_captions]\n",
        "\n",
        "# Initialize a list to store the generated captions\n",
        "generated_captions = []\n",
        "\n",
        "# Generate captions for the test images\n",
        "for image in test_images:\n",
        "    image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "    initial_caption = np.array([[word_index['<start>']]])  # Start token\n",
        "    end_token = word_index['<end>']  # End token\n",
        "\n",
        "    # Generate captions word by word\n",
        "    while True:\n",
        "        predictions = model.predict([image, initial_caption])\n",
        "        predicted_token_index = np.argmax(predictions[0, -1, :])\n",
        "        predicted_word = index_to_word[predicted_token_index]\n",
        "        initial_caption = np.append(initial_caption, [[predicted_token_index]], axis=1)\n",
        "\n",
        "        # Break if end token is generated or maximum caption length is reached\n",
        "        if predicted_token_index == end_token or initial_caption.shape[1] >= max_caption_length:\n",
        "            break\n",
        "\n",
        "    # Remove start and end tokens and convert token indices to words\n",
        "    generated_caption = initial_caption[0, 1:-1]\n",
        "    generated_caption_words = [index_to_word[token_index] for token_index in generated_caption]\n",
        "\n",
        "    # Add the generated caption to the list\n",
        "    generated_captions.append(generated_caption_words)\n",
        "\n",
        "# Prepare the generated captions in the required format\n",
        "generated_captions = [caption.split() for caption in generated_captions]\n",
        "\n",
        "# Calculate the BLEU score\n",
        "bleu_score = corpus_bleu(reference_captions, generated_captions)\n",
        "\n",
        "# Print the BLEU score\n",
        "print(\"BLEU Score:\", bleu_score)\n"
      ],
      "metadata": {
        "id": "cGXh-HOkhanb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **caption generation**"
      ],
      "metadata": {
        "id": "EusorOxSiH72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = load_model(saved_model_path)\n",
        "\n",
        "# Load the word index\n",
        "with open(os.path.join(processed_dir, 'word_index.json'), 'r') as f:\n",
        "    word_index = json.load(f)\n",
        "\n",
        "# Reverse the word index to map indices back to words\n",
        "index_to_word = {index: word for word, index in word_index.items()}\n",
        "\n",
        "# Load the image you want to caption\n",
        "image_path = 'path_to_image.jpg'\n",
        "image = load_image(image_path)\n",
        "\n",
        "# Preprocess the image\n",
        "image = cv2.resize(image, image_size)\n",
        "image = image.astype(np.float32) / 255.0\n",
        "image = np.expand_dims(image, axis=0)  # Add batch dimension\n",
        "\n",
        "# Generate the caption for the image\n",
        "initial_caption = np.array([[word_index['<start>']]])  # Start token\n",
        "end_token = word_index['<end>']  # End token\n",
        "\n",
        "# Generate captions word by word\n",
        "while True:\n",
        "    predictions = model.predict([image, initial_caption])\n",
        "    predicted_token_index = np.argmax(predictions[0, -1, :])\n",
        "    predicted_word = index_to_word[predicted_token_index]\n",
        "    initial_caption = np.append(initial_caption, [[predicted_token_index]], axis=1)\n",
        "\n",
        "    # Break if end token is generated or maximum caption length is reached\n",
        "    if predicted_token_index == end_token or initial_caption.shape[1] >= max_caption_length:\n",
        "        break\n",
        "\n",
        "# Remove start and end tokens and convert token indices to words\n",
        "generated_caption = initial_caption[0, 1:-1]\n",
        "generated_caption_words = [index_to_word[token_index] for token_index in generated_caption]\n",
        "\n",
        "# Convert the generated caption words into a single string\n",
        "generated_caption_string = ' '.join(generated_caption_words)\n",
        "\n",
        "# Print the generated caption\n",
        "print(\"Generated Caption:\", generated_caption_string)\n"
      ],
      "metadata": {
        "id": "71UehRcBiGe5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**continue with the code to train the model and improve the captioning system:**"
      ],
      "metadata": {
        "id": "GXTpl162i0Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "# Load the pre-trained CNN model (VGG16)\n",
        "cnn_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the layers of the CNN model\n",
        "for layer in cnn_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Define the input layers for images and captions\n",
        "image_input = Input(shape=(224, 224, 3))\n",
        "caption_input = Input(shape=(max_caption_length,))\n",
        "\n",
        "# Extract features from the images using the CNN model\n",
        "image_features = cnn_model(image_input)\n",
        "\n",
        "# Define the caption embedding layer\n",
        "caption_embedding = Embedding(input_dim=vocab_size, output_dim=256, input_length=max_caption_length)(caption_input)\n",
        "\n",
        "# Define the LSTM layer for captions\n",
        "caption_lstm = LSTM(256)(caption_embedding)\n",
        "\n",
        "# Apply attention mechanism between image features and captions\n",
        "attention = Attention()([image_features, caption_lstm])\n",
        "\n",
        "# Concatenate the image features and attention output\n",
        "concatenation = tf.keras.layers.Concatenate()([image_features, attention])\n",
        "\n",
        "# Generate the output caption\n",
        "output_caption = Dense(units=vocab_size, activation='softmax')(concatenation)\n",
        "\n",
        "# Define the model\n",
        "model = Model(inputs=[image_input, caption_input], outputs=output_caption)\n",
        "\n",
        "# Compile the model with appropriate loss function and optimizer\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train the model using the data generators\n",
        "model.fit(train_generator, epochs=10, validation_data=val_generator)\n",
        "\n",
        "# Save the trained model\n",
        "model.save('image_caption_model.h5')\n"
      ],
      "metadata": {
        "id": "YVVen1V8i0rj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Generation of captions for new images using pretrained the model."
      ],
      "metadata": {
        "id": "zs83YP54jcP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the trained model\n",
        "model = load_model('image_caption_model.h5')\n",
        "\n",
        "# Load the word index\n",
        "with open(os.path.join(processed_dir, 'word_index.json'), 'r') as f:\n",
        "    word_index = json.load(f)\n",
        "\n",
        "# Reverse the word index to convert indices back to words\n",
        "index_word = {index: word for word, index in word_index.items()}\n",
        "\n",
        "# Load and preprocess a new image\n",
        "new_image = load_image('path/to/new/image.jpg')\n",
        "new_image = np.expand_dims(new_image, axis=0)\n",
        "\n",
        "# Generate the caption for the new image\n",
        "predicted_caption = generate_caption(model, new_image, index_word, max_caption_length)\n",
        "\n",
        "# Print the predicted caption\n",
        "print('Predicted caption:', predicted_caption)\n",
        "\n",
        "# Display the new image\n",
        "plt.imshow(new_image.squeeze())\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "vMUOmt9ujckl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Re-Verification**"
      ],
      "metadata": {
        "id": "OQzWpGbikZ9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the trained model\n",
        "model = load_model('image_caption_model.h5')\n",
        "\n",
        "# Load the word index\n",
        "with open(os.path.join(processed_dir, 'word_index.json'), 'r') as f:\n",
        "    word_index = json.load(f)\n",
        "\n",
        "# Reverse the word index to convert indices back to words\n",
        "index_word = {index: word for word, index in word_index.items()}\n",
        "\n",
        "# Set the maximum length for generating captions during evaluation\n",
        "max_caption_length = 50\n",
        "\n",
        "# Initialize lists to store the reference captions and generated captions\n",
        "references = []\n",
        "predictions = []\n",
        "\n",
        "# Generate captions for the validation images\n",
        "for i in range(len(val_generator)):\n",
        "    batch_images, batch_captions = val_generator[i]\n",
        "    batch_size = batch_images.shape[0]\n",
        "\n",
        "    # Generate captions for each image in the batch\n",
        "    for j in range(batch_size):\n",
        "        image = np.expand_dims(batch_images[j], axis=0)\n",
        "        caption = generate_caption(model, image, index_word, max_caption_length)\n",
        "\n",
        "        # Convert the tokenized caption back to words\n",
        "        reference = ' '.join([index_word[index] for index in batch_captions[j] if index != 0])\n",
        "        prediction = ' '.join(caption.split()[1:-1])  # Remove start and end tokens\n",
        "\n",
        "        references.append(reference)\n",
        "        predictions.append(prediction)\n",
        "\n",
        "# Calculate evaluation metric (e.g., BLEU score)\n",
        "bleu_score = calculate_bleu_score(references, predictions)\n",
        "\n",
        "# Print the evaluation metric\n",
        "print('BLEU score:', bleu_score)\n"
      ],
      "metadata": {
        "id": "5H7DxylikaOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Set the paths to the image directories\n",
        "train_dir = '/content/IMAGE CAPTIONS/coco2017/train2017'\n",
        "val_dir = '/content/IMAGE CAPTIONS/coco2017/val2017'\n",
        "test_dir = '/content/IMAGE CAPTIONS/coco2017/test2017'\n",
        "\n",
        "# Set the desired image size\n",
        "image_size = (224, 224)\n",
        "\n",
        "# Preprocess the images\n",
        "def preprocess_images(image_dir):\n",
        "    images = []\n",
        "    for image_file in os.listdir(image_dir):\n",
        "        image_path = os.path.join(image_dir, image_file)\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        image = cv2.resize(image, image_size)\n",
        "        image = image.astype(np.float32) / 255.0\n",
        "        images.append(image)\n",
        "    return np.array(images)\n",
        "\n",
        "# Preprocess images in the train directory\n",
        "train_images = preprocess_images(train_dir)\n",
        "\n",
        "# Preprocess images in the validation directory\n",
        "val_images = preprocess_images(val_dir)\n",
        "\n",
        "# Preprocess images in the test directory\n",
        "test_images = preprocess_images(test_dir)\n",
        "\n",
        "# Print the shapes of the preprocessed images\n",
        "print('Train images shape:', train_images.shape)\n",
        "print('Validation images shape:', val_images.shape)\n",
        "print('Test images shape:', test_images.shape)\n"
      ],
      "metadata": {
        "id": "N8GNfll60DE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODHxTOxA2gBG3oa6BHDLrM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}